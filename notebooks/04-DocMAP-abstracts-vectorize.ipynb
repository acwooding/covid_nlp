{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet, start by setting up your environment and datasets by following the instructions in the README. It should be something like:\n",
    "* `make create_environment`\n",
    "* `conda activate covid_nlp`\n",
    "* `make update_environment`\n",
    "* `make data`\n",
    "\n",
    "Several common packages that you may want to use (e.g. UMAP, HDBSCAN, enstop, sklearn) have already been added to the `covid_nlp` environment via `environment.yml`. To add more, edit that file and do a:\n",
    "  ` make update_environment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embedding of abstracts\n",
    "In this notebook we'll follow https://github.com/gclen/covid19-kaggle/blob/master/embed_abstracts_interactive.ipynb to embed abstracts. It uses the the techniques from https://umap-learn.readthedocs.io/en/latest/document_embedding.html to embed documents using UMAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically pick up code changes in the `src` module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports from easydata\n",
    "from src import paths\n",
    "from src.data import Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other packages\n",
    "from scipy import sparse\n",
    "\n",
    "# tokenizing/vectorizing\n",
    "from src.data.em_method import em_sparse\n",
    "import en_core_sci_sm # A full spaCy pipeline for biomedical data.\n",
    "from scispacy.custom_tokenizer import combined_rule_tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# dimension reduction\n",
    "import umap\n",
    "import umap.plot\n",
    "# clustering\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the dataset\n",
    "\n",
    "The metadata has been augmented with where the files can be found relative to `paths[\"interim_data_path\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths['interim_data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returned an empty list, go back and re-run `make data` as described at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid_nlp_20200319'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "meta_ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_ds.DESCR[:457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processed dataframe is the `data` method of this data source \n",
    "meta_df = meta_ds.data\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally filter it down to published papers with a cc-by license\n",
    "\n",
    "meta_df.file_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[(meta_df.file_type=='comm_use_subset') | (meta_df.file_type=='noncomm_use_subset')].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics on the dataset\n",
    "\n",
    "The JSON files given in the `path` column of the metadata dataframe are the papers in `json` format (as dicts)\n",
    "that include the following keys:\n",
    "* `paper_id`\n",
    "* `metadata`\n",
    "* `abstract`\n",
    "* `body_text`\n",
    "* `bib_entries`\n",
    "* `ref_entries`\n",
    "* `back_matter`\n",
    "\n",
    "where the `paper_id` is the sha hash from the medadata.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths['interim_data_path'] / ds_name / meta_df['path'][0]\n",
    "file = json.load(open(filename, 'rb'))\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = meta_df.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten abstracts for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_abstracts = [a[:140] for a in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['abstract_length'] = meta_df.abstract.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df = meta_df[meta_df.abstract_length > 0].reset_index()\n",
    "hover_df['short_abstracts'] = short_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize to get word-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tfidf with L1 normalize + Estimation Maximization to get rid of the \"averages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(min_df=5, norm='l1', tokenizer=spacy_tokenizer)\n",
    "word_doc_matrix = vectorizer.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_matrix, weights = em_sparse(word_doc_matrix, prior_noise=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimension using UMAP and Hellinger distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_2d = umap.UMAP(n_components=2, n_neighbors=10,\n",
    "                         metric='hellinger',\n",
    "                         random_state=42).fit(word_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15)\n",
    "clusterer.fit_predict(embedding_2d.embedding_)\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hover_df.cluster.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_cols = ['title', 'file_type', 'short_abstracts', 'cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = umap.plot.interactive(embedding_2d, labels=hover_df['cluster'],\n",
    "                          hover_data=hover_df.reset_index()[hover_cols]);\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/figures/04-DocMAP-abstracts.png\" alt=\"DocMAP embedding visualization\" title=\"DocMAP embedding visualization\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank points based on distance to a representative point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import RankedPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = RankedPoints(embedding_2d.embedding_, clusterer, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.calculate_all_distances_to_center()\n",
    "examples.get_all_cluster_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['rank_in_cluster'] = examples.embedding_df['rank_in_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_papers = {}\n",
    "grouped_by_cluster = hover_df.groupby('cluster')\n",
    "\n",
    "for cluster_id, group in grouped_by_cluster:\n",
    "    top_papers = group.sort_values('rank_in_cluster', ascending=True).head(5)\n",
    "    \n",
    "    top_5_papers[int(cluster_id)] = '<ol>' + ''.join([f'<li><a href=\"{r.doi}\">{r.title}</a></li>' for _, r in top_papers.iterrows()]) + '</ol>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_papers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li><a href=\"http://dx.doi.org/10.1155/2011/609465\">Feline and Canine Coronaviruses: Common Genetic and Pathobiological Features</a></li><li><a href=\"http://dx.doi.org/10.3390/v11111068\">Diagnosis of Feline Infectious Peritonitis: A Review of the Current Literature</a></li><li><a href=\"http://dx.doi.org/10.1186/s12917-017-1147-8\">Sensitivity and specificity of a real-time reverse transcriptase polymerase chain reaction detecting feline coronavirus mutations in effusion and serum/plasma of cats to diagnose feline infectious peritonitis</a></li><li><a href=\"http://dx.doi.org/10.1038/srep20022\">Experimental feline enteric coronavirus infection reveals an aberrant infection pattern and shedding of mutants with impaired infectivity in enterocyte cultures</a></li><li><a href=\"http://dx.doi.org/10.1292/jvms.19-0090\">Feline coronavirus isolates from a part of Brazil: insights into molecular epidemiology and phylogeny inferred from the 7b gene</a></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_papers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li><a href=\"http://dx.doi.org/10.3389/fphar.2016.00146\">Protein Kinase C-Î´ Mediates Shedding of Angiotensin-Converting Enzyme 2 from Proximal Tubular Cells</a></li><li><a href=\"http://dx.doi.org/10.1371/journal.pone.0034747\">Angiotensin Converting Enzyme (ACE) and ACE2 Bind Integrins and ACE2 Regulates Integrin Signalling</a></li><li><a href=\"http://dx.doi.org/10.1186/1471-2164-8-194\">Identification and characterisation of the angiotensin converting enzyme-3 (ACE3) gene: a novel mammalian homologue of ACE</a></li><li><a href=\"http://dx.doi.org/10.3390/biom9120886\">Novel Variants of Angiotensin Converting Enzyme-2 of Shorter Molecular Size to Target the Kidney Renin Angiotensin System</a></li><li><a href=\"http://dx.doi.org/10.1371/journal.pone.0150861\">Anti-Inflammatory Action of Angiotensin 1-7 in Experimental Colitis</a></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can now be used as input to an interactive plot\n",
    "\n",
    "See https://github.com/gclen/covid19-kaggle for an example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_nlp]",
   "language": "python",
   "name": "conda-env-covid_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
