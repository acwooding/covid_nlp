{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet, start by setting up your environment and datasets by following the instructions in the README. It should be something like:\n",
    "* `make create_environment`\n",
    "* `conda activate covid_nlp`\n",
    "* `make update_environment`\n",
    "* `make data`\n",
    "\n",
    "Several common packages that you may want to use (e.g. UMAP, HDBSCAN, enstop, sklearn) have already been added to the `covid_nlp` environment via `environment.yml`. To add more, edit that file and do a:\n",
    "  ` make update_environment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embedding of abstracts\n",
    "In this notebook we'll follow https://github.com/gclen/covid19-kaggle/blob/master/embed_abstracts_interactive.ipynb to embed abstracts. It uses the the techniques from https://umap-learn.readthedocs.io/en/latest/document_embedding.html to embed documents using UMAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically pick up code changes in the `src` module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports from easydata\n",
    "from src import paths\n",
    "from src.data import Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other packages\n",
    "from scipy import sparse\n",
    "\n",
    "# tokenizing/vectorizing\n",
    "import en_core_sci_sm # A full spaCy pipeline for biomedical data.\n",
    "from scispacy.custom_tokenizer import combined_rule_tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# dimension reduction\n",
    "import umap\n",
    "import umap.plot\n",
    "# clustering\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the dataset\n",
    "\n",
    "The metadata has been augmented with where the files can be found relative to `paths[\"interim_data_path\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths['interim_data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returned an empty list, go back and re-run `make data` as described at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid_nlp_20200319'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "meta_ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_ds.DESCR[:457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processed dataframe is the `data` method of this data source \n",
    "meta_df = meta_ds.data\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally filter it down to published papers with a cc-by license\n",
    "\n",
    "meta_df.file_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[(meta_df.file_type=='comm_use_subset') | (meta_df.file_type=='noncomm_use_subset')].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics on the dataset\n",
    "\n",
    "The JSON files given in the `path` column of the metadata dataframe are the papers in `json` format (as dicts)\n",
    "that include the following keys:\n",
    "* `paper_id`\n",
    "* `metadata`\n",
    "* `abstract`\n",
    "* `body_text`\n",
    "* `bib_entries`\n",
    "* `ref_entries`\n",
    "* `back_matter`\n",
    "\n",
    "where the `paper_id` is the sha hash from the medadata.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths['interim_data_path'] / ds_name / meta_df['path'][0]\n",
    "file = json.load(open(filename, 'rb'))\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = meta_df.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the abstracts (initial embedding)\n",
    "\n",
    "#### Set up tokenizer\n",
    "\n",
    "We'll use scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_sci_sm.load()\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just want the tokenization !\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize to get word-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(min_df=5, tokenizer=spacy_tokenizer)\n",
    "word_doc_matrix = vectorizer.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = paths['processed_data_path'] / \"spacy-abstract-matrix-cc-by.npz\"\n",
    "#save_path = paths['processed_data_path'] / \"spacy-abstract-matrix.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse.save_npz(save_path, word_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX Split the notebook here eventually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_matrix = sparse.load_npz(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimension using UMAP and Hellinger distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_2d = umap.UMAP(n_components=2, n_neighbors=10,\n",
    "                         metric='hellinger', init='random',\n",
    "                         random_state=42).fit(word_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX Replace with vectorizer code  to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX EM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten abstracts for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_abstracts = [a[:140] for a in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df = meta_df[meta_df.abstract_length > 0].reset_index()\n",
    "hover_df['short_abstracts'] = short_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15)\n",
    "clusterer.fit_predict(embedding_2d.embedding_)\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_cols = ['title', 'file_type', 'short_abstracts', 'cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = umap.plot.interactive(embedding_2d, labels=hover_df['cluster'],\n",
    "                          hover_data=hover_df.reset_index()[hover_cols]);\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank points based on distance to a representative point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import RankedPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = RankedPoints(embedding_2d.embedding_, clusterer, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.calculate_all_distances_to_center()\n",
    "examples.get_all_cluster_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['rank_in_cluster'] = examples.embedding_df['rank_in_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_papers = {}\n",
    "grouped_by_cluster = hover_df.groupby('cluster')\n",
    "\n",
    "for cluster_id, group in grouped_by_cluster:\n",
    "    top_papers = group.sort_values('rank_in_cluster', ascending=True).head(5)\n",
    "    \n",
    "    top_5_papers[int(cluster_id)] = '<ol>' + ''.join([f'<li><a href=\"{r.doi}\">{r.title}</a></li>' for _, r in top_papers.iterrows()]) + '</ol>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_papers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li><a href=\"http://dx.doi.org/10.1371/journal.pbio.0030172\">A Three-Stemmed mRNA Pseudoknot in the SARS Coronavirus Frameshift Signal</a></li><li><a href=\"http://dx.doi.org/10.1093/nar/gkw480\">A novel role for poly(C) binding proteins in programmed ribosomal frameshifting</a></li><li><a href=\"http://dx.doi.org/10.1093/nar/gkr686\">mRNA pseudoknot structures can act as ribosomal roadblocks</a></li><li><a href=\"http://dx.doi.org/10.1093/nar/gkl1033\">Identification of functional, endogenous programmed −1 ribosomal frameshift signals in the genome of Saccharomyces cerevisiae</a></li><li><a href=\"http://dx.doi.org/10.1371/journal.pone.0062283\">Regulation of Programmed Ribosomal Frameshifting by Co-Translational Refolding RNA Hairpins</a></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can now be used as input to an interactive plot\n",
    "\n",
    "See https://github.com/gclen/covid19-kaggle for an example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_nlp] *",
   "language": "python",
   "name": "conda-env-covid_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
