{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet, start by setting up your environment and datasets by following the instructions in the README. It should be something like:\n",
    "* `make create_environment`\n",
    "* `conda activate covid_nlp`\n",
    "* `make update_environment`\n",
    "* `make data`\n",
    "\n",
    "Several common packages that you may want to use (e.g. UMAP, HDBSCAN, enstop, sklearn) have already been added to the `covid_nlp` environment via `environment.yml`. To add more, edit that file and do a:\n",
    "  ` make update_environment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embedding of abstracts\n",
    "In this notebook we'll follow https://github.com/ddangelov/Top2Vec/blob/master/notebooks/top2vec_covid19_example.ipynb to embed abstracts using https://github.com/ddangelov/Top2Vec. \n",
    "\n",
    "The inital work was done on the inidividual sections of the papers, here we only use abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically pick up code changes in the `src` module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports from easydata\n",
    "from src import paths\n",
    "from src.data import Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other packages\n",
    "\n",
    "# embedding + clustering\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the dataset\n",
    "\n",
    "The metadata has been augmented with where the files can be found relative to `paths[\"interim_data_path\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths['interim_data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returned an empty list, go back and re-run `make data` as described at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid_nlp_20200319'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "meta_ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_ds.DESCR[:457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processed dataframe is the `data` method of this data source \n",
    "meta_df = meta_ds.data\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter it down to published papers with a cc-by license\n",
    "\n",
    "meta_df.file_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[(meta_df.file_type=='comm_use_subset') | (meta_df.file_type=='noncomm_use_subset')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics on the dataset\n",
    "\n",
    "The JSON files given in the `path` column of the metadata dataframe are the papers in `json` format (as dicts)\n",
    "that include the following keys:\n",
    "* `paper_id`\n",
    "* `metadata`\n",
    "* `abstract`\n",
    "* `body_text`\n",
    "* `bib_entries`\n",
    "* `ref_entries`\n",
    "* `back_matter`\n",
    "\n",
    "where the `paper_id` is the sha hash from the medadata.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths['interim_data_path'] / ds_name / meta_df['path'][0]\n",
    "file = json.load(open(filename, 'rb'))\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = meta_df.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Top2Vec Model\n",
    "\n",
    "Create a joint word and document embedding with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train doc2vec model\n",
    "train_corpus = []\n",
    "\n",
    "for index, abstract in enumerate(abstracts):\n",
    "    train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(abstract), [index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = gensim.models.doc2vec.Doc2Vec(documents=train_corpus, vector_size=300, min_count=50,\n",
    "                                      window=15, sample=10e-5, negative=5, hs=0, workers=80,\n",
    "                                      epochs=40, dm=0, dbow_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"top2vec_abstracts\"\n",
    "path = paths['processed_data_path'] / model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX optional break point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimension with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix = np.vstack([model.docvecs[i] for i in range(model.docvecs.count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap_model_2D = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=42).fit(doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom').fit(umap_model_2D.embedding_)\n",
    "labels = pd.Series(cluster.labels_)\n",
    "print(\"Number of -1 labels: \", len(labels[labels==-1])/len(labels))\n",
    "print(\"Number of clusters: \", len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clust = umap.plot.interactive(umap_model_2D,labels=labels, theme='darkgreen')\n",
    "show(fig_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(abstracts, columns=['abstract']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top2vec vectors and topic words \n",
    "# \n",
    "# Topic vectors are centroids of dense areas of documents\n",
    "# Topic words are word vectors closest to topic vector\n",
    "\n",
    "doc_group_mapping = data_df.copy()\n",
    "doc_group_mapping[\"label\"] = labels\n",
    "\n",
    "topic_vectors = []\n",
    "sim_words_l = []\n",
    "top_words_l = []\n",
    "topic_index = 0\n",
    "\n",
    "# remove outlier documents as they are noise \n",
    "lables_list = list(set(labels))\n",
    "lables_list.remove(-1)\n",
    "\n",
    "for group in lables_list:\n",
    "    \n",
    "    # generate topic vector\n",
    "    topic_vector = [0]*300\n",
    "    vec_indices = doc_group_mapping[doc_group_mapping.label==group].index.tolist()\n",
    "    for vec_index in vec_indices:\n",
    "           topic_vector = topic_vector + model.docvecs[vec_index]\n",
    "    topic_vector = topic_vector/len(vec_indices)\n",
    "    \n",
    "    topic_vectors.append(topic_vector)\n",
    "    \n",
    "    \n",
    "    # find closest word vectors to topic vector\n",
    "    sim_words = model.most_similar(positive=[topic_vector], topn=50)\n",
    "    sim_words_l.append(sim_words)\n",
    "    top_words_l.append([word[0] for word in sim_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word cloud for topic \n",
    "def generate_wordcloud(top_words, top_num):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(WordCloud(width=1600, height=400, background_color='black').generate_from_frequencies(dict(top_words)), interpolation='bilinear');\n",
    "    plt.title(\"Topic \" + str(top_num), loc='left', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# order topics by size(number of documents in dense cluster)\n",
    "label_df = pd.DataFrame(doc_group_mapping[\"label\"].value_counts()).sort_values(by=\"label\", ascending=False)\n",
    "lables_list = list(label_df.index)\n",
    "lables_list.remove(-1)\n",
    "\n",
    "for group in lables_list[0:20]:\n",
    "    generate_wordcloud(sim_words_l[group], group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to Search by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_nlp] *",
   "language": "python",
   "name": "conda-env-covid_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
