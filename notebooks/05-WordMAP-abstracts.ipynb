{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet, start by setting up your environment and datasets by following the instructions in the README. It should be something like:\n",
    "* `make create_environment`\n",
    "* `conda activate covid_nlp`\n",
    "* `make update_environment`\n",
    "* `make data`\n",
    "\n",
    "Several common packages that you may want to use (e.g. UMAP, HDBSCAN, enstop, sklearn) have already been added to the `covid_nlp` environment via `environment.yml`. To add more, edit that file and do a:\n",
    "  ` make update_environment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically pick up code changes in the `src` module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports from easydata\n",
    "from src import paths\n",
    "from src.data import Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.numba_word_vectorizer import word_word_cooccurence_matrix\n",
    "from src.data.em_method import em_sparse\n",
    "from src.utils import RankedPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from enstop import PLSA\n",
    "import umap\n",
    "import umap.plot\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the dataset\n",
    "\n",
    "The metadata has been augmented with where the files can be found relative to `paths[\"interim_data_path\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returned an empty list, go back and re-run `make data` as described at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid_nlp_20200319'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "meta_ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_ds.DESCR[:457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processed dataframe is the `data` method of this data source \n",
    "meta_df = meta_ds.data\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics on the dataset\n",
    "\n",
    "The JSON files given in the `path` column of the metadata dataframe are the papers in `json` format (as dicts)\n",
    "that include the following keys:\n",
    "* `paper_id`\n",
    "* `metadata`\n",
    "* `abstract`\n",
    "* `body_text`\n",
    "* `bib_entries`\n",
    "* `ref_entries`\n",
    "* `back_matter`\n",
    "\n",
    "where the `paper_id` is the sha hash from the medadata.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths['interim_data_path'] / ds_name / meta_df['path'][0]\n",
    "file = json.load(open(filename, 'rb'))\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = meta_df.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten abstracts for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_length = 140\n",
    "short_abstracts = [a[:max_abs_length] for a in abstracts]\n",
    "meta_df['abstract_length'] = meta_df.abstract.str.len()\n",
    "data_df = meta_df[meta_df.abstract_length > 0].reset_index()\n",
    "data_df['short_abstracts'] = short_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXXX Hack around a zero row in the word matrix coming out of word_word_cooccurence_matrix\n",
    "EM doesn't handle zero rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df.abstract.str.contains(\"subsp\")].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = data_df.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial vectorization to the word-word matrix\n",
    "\n",
    "This replaces the normal CountVectorizer step from TfidfVectorizer (CountVectorizer+TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_word_matrix, token_to_index, index_to_token = word_word_cooccurence_matrix(raw_text, min_df=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of the word matrix\n",
    "word_array = np.array([index_to_token[x] for x in range(raw_word_matrix.shape[0])])\n",
    "hover_df = pd.DataFrame(word_array, columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without the above hack we get a zero row...\n",
    "zero_rows = np.where(raw_word_matrix.getnnz(1)==0)[0]\n",
    "len(zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_matrix_before = TfidfTransformer(norm='l1').fit_transform(raw_word_matrix)\n",
    "word_matrix_after = TfidfTransformer(norm='l1').fit_transform(raw_word_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_word_matrix = normalize(sparse.hstack([word_matrix_before, word_matrix_after]), norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_prior = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_matrix_before, w_before = em_sparse(word_matrix_before, prior_noise=background_prior)\n",
    "word_matrix_after, w_after = em_sparse(word_matrix_after, prior_noise=background_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = normalize(sparse.hstack([word_matrix_before, word_matrix_after]), norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dimension = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicer = PLSA(n_components=topic_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "topicer.fit(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_topic = topicer.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduce with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = umap.UMAP(n_components=2, n_neighbors=10, random_state=42, metric='hellinger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_2d = mapping.fit(word_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "clusterer.fit_predict(embedding_2d.embedding_)\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['cluster'] = labels\n",
    "value_counts = hover_df.cluster.value_counts()\n",
    "print(f\"Number of clusters: {len(value_counts)}\")\n",
    "print(f\"Cluster value counts:\\n{value_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = umap.plot.interactive(embedding_2d, labels=hover_df['cluster'],\n",
    "                          hover_data=hover_df, point_size=3);\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/figures/05-WordMAP-abstracts.png\" alt=\"WordMAP embedding visualization\" title=\"WordMAP embedding visualization\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank points based on distance to a representative point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = RankedPoints(embedding_2d.embedding_, clusterer, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.calculate_all_distances_to_center()\n",
    "examples.get_all_cluster_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['rank_in_cluster'] = examples.embedding_df['rank_in_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "top_cluster_points = {}\n",
    "top_cluster_points_freq = {}\n",
    "\n",
    "grouped_by_cluster = hover_df.groupby('cluster')\n",
    "\n",
    "for cluster_id, group in grouped_by_cluster:\n",
    "    top_points = group.sort_values('rank_in_cluster', ascending=True).head(num_points)\n",
    "    top_points['inverse_rank'] = top_points.rank_in_cluster.apply(lambda x: num_points - x)\n",
    "    top_cluster_points_freq[int(cluster_id)] = dict(zip(top_points.word, top_points.inverse_rank))\n",
    "    top_cluster_points[int(cluster_id)] = '<ol>' + ''.join([f'<li>{r.word}</li>' for _, r in top_points.head(min_cluster_size).iterrows()]) + '</ol>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word clouds based on ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word cloud for word topic \n",
    "def generate_wordcloud(topic_words, topic_num):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(WordCloud(width=1600, height=400, background_color='black').generate_from_frequencies(topic_words))\n",
    "    plt.title(\"Topic \" + str(topic_num), loc='left', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cluster_points[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li>32</li><li>sd</li><li>133</li><li>79</li><li>320</li><li>46</li><li>115</li><li>85</li><li>107</li><li>112</li><li>18</li><li>72</li><li>64</li><li>38</li><li>39</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(top_cluster_points_freq[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/figures/05-WordMAP-topic1.png\" title=\"WordMAP topic 1 visualization\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View largest word topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "top_clusters = value_counts.index[1:num_topics + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in top_clusters:\n",
    "    generate_wordcloud(top_cluster_points_freq[cluster], cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_nlp]",
   "language": "python",
   "name": "conda-env-covid_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
