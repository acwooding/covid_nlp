{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet, start by setting up your environment and datasets by following the instructions in the README. It should be something like:\n",
    "* `make create_environment`\n",
    "* `conda activate covid_nlp`\n",
    "* `make update_environment`\n",
    "* `make data`\n",
    "\n",
    "Several common packages that you may want to use (e.g. UMAP, HDBSCAN, enstop, sklearn) have already been added to the `covid_nlp` environment via `environment.yml`. To add more, edit that file and do a:\n",
    "  ` make update_environment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically pick up code changes in the `src` module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports from easydata\n",
    "from src import paths\n",
    "from src.data import Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.numba_word_vectorizer import word_word_cooccurence_matrix\n",
    "from src.data.em_method import em_sparse\n",
    "from src.utils import RankedPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.em_method import em_sparse\n",
    "import en_core_sci_sm # A full spaCy pipeline for biomedical data.\n",
    "from scispacy.custom_tokenizer import combined_rule_tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from enstop import PLSA\n",
    "import umap\n",
    "import umap.plot\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the dataset\n",
    "\n",
    "The metadata has been augmented with where the files can be found relative to `paths[\"interim_data_path\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell returned an empty list, go back and re-run `make data` as described at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid_nlp_20200319'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "meta_ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_ds.DESCR[:457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processed dataframe is the `data` method of this data source \n",
    "meta_df = meta_ds.data\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics on the dataset\n",
    "\n",
    "The JSON files given in the `path` column of the metadata dataframe are the papers in `json` format (as dicts)\n",
    "that include the following keys:\n",
    "* `paper_id`\n",
    "* `metadata`\n",
    "* `abstract`\n",
    "* `body_text`\n",
    "* `bib_entries`\n",
    "* `ref_entries`\n",
    "* `back_matter`\n",
    "\n",
    "where the `paper_id` is the sha hash from the medadata.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths['interim_data_path'] / ds_name / meta_df['path'][0]\n",
    "file = json.load(open(filename, 'rb'))\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = meta_df.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten abstracts for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_length = 140\n",
    "short_abstracts = [a[:max_abs_length] for a in abstracts]\n",
    "meta_df['abstract_length'] = meta_df.abstract.str.len()\n",
    "data_df = meta_df[meta_df.abstract_length > 0].reset_index()\n",
    "data_df['short_abstracts'] = short_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXXX Hack around a zero row in the word matrix coming out of word_word_cooccurence_matrix\n",
    "EM doesn't handle zero rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df.abstract.str.contains(\"subsp\")].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word and document matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = data_df.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Word Matrix\n",
    "As in [05-WordMAP-abstracts.ipynb](05-WordMAP-abstracts.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_word_matrix, token_to_index, index_to_token = word_word_cooccurence_matrix(raw_text, min_df=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of the word matrix\n",
    "word_array = np.array([index_to_token[x] for x in range(raw_word_matrix.shape[0])])\n",
    "word_hover_df = pd.DataFrame(word_array, columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without the above hack we get a zero row...\n",
    "zero_rows = np.where(raw_word_matrix.getnnz(1)==0)[0]\n",
    "len(zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_matrix_before = TfidfTransformer(norm='l1').fit_transform(raw_word_matrix)\n",
    "word_matrix_after = TfidfTransformer(norm='l1').fit_transform(raw_word_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_word_matrix = normalize(sparse.hstack([word_matrix_before, word_matrix_after]), norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_prior = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_matrix_before, w_before = em_sparse(word_matrix_before, prior_noise=background_prior)\n",
    "word_matrix_after, w_after = em_sparse(word_matrix_after, prior_noise=background_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = normalize(sparse.hstack([word_matrix_before, word_matrix_after]), norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document Matrix\n",
    "As in [04-DocMAP-abstracts.ipynb](04-WordMAP-abstracts.ipynb), but based on the word vocabulary from above, and using NLTK instead of scispacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_doc_matrix = TfidfVectorizer(vocabulary=token_to_index, norm='l1', min_df=5).fit_transform(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix, weights = em_sparse(raw_doc_matrix, prior_noise=background_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dimension = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicer = PLSA(n_components=topic_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "topicer.fit(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_topic = topicer.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Document Matrix to have the same basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_doc_matrix = doc_matrix * word_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We averaged again by doing a matvec. Do EM again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_doc_matrix, ld_weights = em_sparse(sparse.csr_matrix(low_doc_matrix), prior_noise=background_prior)\n",
    "low_doc_matrix = low_doc_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine to a single word-doc matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_matrix = np.vstack([word_by_topic, low_doc_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduce with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_2d = umap.UMAP(n_components=2, n_neighbors=15,\n",
    "                         metric='hellinger', init='random',\n",
    "                         random_state=42).fit(word_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort out labels for joint embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_support_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = [get_support_index(doc_matrix.getrow(i)) for i in range(doc_matrix.shape[0])]\n",
    "supported_words_array = np.array([\" \".join([index_to_token[index_list[i]] for i in range(len(index_list))]) for index_list in col_indices])\n",
    "doc_hover_df = data_df[['title', 'short_abstracts', 'doi']].copy()\n",
    "doc_hover_df['word'] = supported_words_array\n",
    "doc_hover_df['kind'] = ['doc'] * len(doc_hover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_hover_df['kind'] = ['word'] * len(word_hover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df = word_hover_df.merge(doc_hover_df, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = umap.plot.interactive(embedding_2d, hover_data=hover_df[['word', 'kind', 'title', 'short_abstracts']],\n",
    "                          labels=hover_df['kind'], width=800, height=800, point_size=1);\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/figures/06-topicMAP-abstracts-joint.png\" title=\"Joint embedding visualization\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "Naively cluster words and docs to see what comes out for topic words.\n",
    "\n",
    "XXXX Later cluster docs, take centroids to get topic words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "clusterer.fit_predict(embedding_2d.embedding_)\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['cluster'] = labels\n",
    "value_counts = hover_df.cluster.value_counts()\n",
    "print(f\"Number of clusters: {len(value_counts)}\")\n",
    "print(f\"Cluster value counts:\\n{value_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = umap.plot.interactive(embedding_2d, labels=hover_df['cluster'],\n",
    "                          hover_data=hover_df, point_size=1);\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/figures/06-topicMAP-abstracts-clusters.png\" title=\"Joint embedding clusters visualization\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_no_topic_words = len(hover_df.cluster.value_counts()) - len(hover_df[hover_df.kind=='word'].cluster.value_counts())\n",
    "num_no_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_no_docs = len(hover_df.cluster.value_counts()) - len(hover_df[hover_df.kind=='doc'].cluster.value_counts())\n",
    "num_no_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this method, we get 74 document clusters that don't include any word from our vocabulary, and two word clusters with no docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(hover_df.cluster.value_counts().index).difference(set(hover_df[hover_df.kind=='doc'].cluster.value_counts().index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank points based on distance to a representative point\n",
    "\n",
    "Do this naively by ranking words and docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = RankedPoints(embedding_2d.embedding_, clusterer, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.calculate_all_distances_to_center()\n",
    "examples.get_all_cluster_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df['rank_in_cluster'] = examples.embedding_df['rank_in_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top words in each cluster (that has words in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 30\n",
    "top_cluster_points = {}\n",
    "top_cluster_points_freq = {}\n",
    "\n",
    "grouped_by_cluster = hover_df[hover_df.kind=='word'][['word', 'rank_in_cluster', 'cluster']].groupby('cluster')\n",
    "\n",
    "for cluster_id, group in grouped_by_cluster:\n",
    "    top_points = group.sort_values('rank_in_cluster', ascending=True).head(num_points)\n",
    "    inverse_rank = list(range(1, len(top_points)+1))[::-1]\n",
    "    top_points['inverse_rank'] = inverse_rank\n",
    "    top_cluster_points_freq[int(cluster_id)] = dict(zip(top_points.word, top_points.inverse_rank))\n",
    "    top_cluster_points[int(cluster_id)] = '<ol>' + ''.join([f'<li>{r.word}</li>' for _, r in top_points.head(min_cluster_size).iterrows()]) + '</ol>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top docs in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "top_cluster_docs = {}\n",
    "\n",
    "grouped_by_cluster = hover_df[hover_df.kind=='doc'][['word', 'title', 'doi', 'rank_in_cluster', 'cluster']].groupby('cluster')\n",
    "\n",
    "for cluster_id, group in grouped_by_cluster:\n",
    "    top_points = group.sort_values('rank_in_cluster', ascending=True).head(num_points)\n",
    "\n",
    "    top_cluster_docs[int(cluster_id)] = '<ol>' + ''.join([f'<li><a href=\"{r.doi}\">{r.title}</a></li>' for _, r in top_points.head(min_cluster_size).iterrows()]) + '</ol>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word clouds based on ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word cloud for word topic \n",
    "def generate_wordcloud(topic_words, topic_num):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(WordCloud(width=1600, height=400, background_color='black').generate_from_frequencies(topic_words))\n",
    "    plt.title(\"Topic \" + str(topic_num), loc='left', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(top_cluster_points[cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li>west</li><li>southern</li><li>migrant</li><li>british</li><li>ksa</li><li>iran</li><li>temperate</li><li>republic</li><li>south</li><li>americas</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(top_cluster_points_freq[cluster], cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(top_cluster_docs[cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li><a href=\"http://dx.doi.org/10.3389/fpubh.2018.00385\">Point-Of-Care Testing Curriculum and Accreditation for Public Health—Enabling Preparedness, Response, and Higher Standards of Care at Points of Need</a></li><li><a href=\"10.1101/2020.02.06.20020974\">Clinical analysis of 23 cases of 2019 novel coronavirus infection in Xinyang City, Henan Province</a></li><li><a href=\"http://dx.doi.org/10.1016/j.virol.2009.05.025\">Mouse Adenovirus Type 1 Infection of Macrophages</a></li><li><a href=\"http://dx.doi.org/10.1128/mSphere.00585-18\">Gut Virome Analysis of Cameroonians Reveals High Diversity of Enteric Viruses, Including Potential Interspecies Transmitted Viruses</a></li><li><a href=\"http://dx.doi.org/10.15537/smj.2019.8.24447\">Survival of mechanically ventilated patients admitted to intensive care units: Results from a tertiary care center between 2016-2018</a></li><li><a href=\"http://dx.doi.org/10.1128/JVI.01294-14\">Catalytic Function and Substrate Specificity of the Papain-Like Protease Domain of nsp3 from the Middle East Respiratory Syndrome Coronavirus</a></li><li><a href=\"http://dx.doi.org/10.1126/scitranslmed.aad6873\">Host gene expression classifiers diagnose acute respiratory illness etiology</a></li><li><a href=\"http://dx.doi.org/10.3201/eid2007.140571\">MERS Coronavirus in Dromedary Camel Herd, Saudi Arabia</a></li><li><a href=\"doi.org/10.1101/2020.03.02.20030320\">Preliminary estimation of the novel coronavirus disease (COVID-19) cases in Iran: a modelling analysis based on overseas cases and air travel data</a></li><li><a href=\"http://dx.doi.org/10.3201/eid2007.140296\">Detection and Genetic Characterization of Deltacoronavirus in Pigs, Ohio, USA, 2014</a></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View largest topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "top_clusters = value_counts.index[1:num_topics + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in top_clusters:\n",
    "    generate_wordcloud(top_cluster_points_freq[cluster], cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:covid_nlp]",
   "language": "python",
   "name": "conda-env-covid_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
